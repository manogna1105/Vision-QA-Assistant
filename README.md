# ğŸ§  Vision-QA Assistant (Telegram Bot)

Vision-QA Assistant is a multimodal chatbot powered by a vision-language foundation model that answers questions about images via a Telegram interface. Users can send images and ask natural-language questions â€” either by typing or using voice â€” and receive intelligent answers or descriptive captions. The bot combines computer vision with language understanding to support real-time visual question answering (VQA).

## ğŸ§  Problem Statement

In an era where images carry a wealth of information, interpreting them intelligently in real time remains a challenge. Users often need quick insights from visual data â€” whether it's understanding trends in charts, recognizing objects in photos, or summarizing visual scenes. 

## âœ¨ Features

- ğŸ“¸ Send an image and get automatic captions.
- â“ Ask questions like â€œWhat is the person doing?â€ or â€œHow many objects are there?â€
- ğŸ™ Voice support â€” ask and receive answers via audio.
- ğŸ” Multi-modal: Text + Image + Audio interaction.
- ğŸ¤– Powered by [BLIP (Salesforce)](https://github.com/salesforce/BLIP) for captioning and VQA.

## Create a virtual environment

python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

## Install dependencies

pip install -r requirements.txt
Required libraries include:

* transformers
* torch
* Pillow
* pydub
* gtts
* speechrecognition
* python-telegram-bot
* ffmpeg (must be installed on your system for audio handling)

## Set your Telegram Bot Token

Replace "API_TOKEN" in vqa_telegram_bot.py with your bot's token from BotFather.

BOT_TOKEN = "your_telegram_bot_token"

## â–¶ï¸ Running the Bot

python vqa_telegram_bot.py

The bot will start polling Telegram for messages. You can now interact with it via your Telegram app.

## ğŸ“¸ How to Use

1. Start the bot: /start
2. Send an image (photo or chart).
3. Ask a question about the image:
     * Text: â€œWhat is happening here?â€
     * Voice: Say your question â€” it will be transcribed and answered!
4. The bot responds with both text and voice output.

## ğŸ§± Architecture Overview

* Telegram Bot Interface: Built with python-telegram-bot.
* Vision-Language Model: BLIP (Salesforce) via HuggingFace Transformers.
* Voice Interface: Speech-to-text via SpeechRecognition, text-to-speech via gTTS.
* Audio Handling: pydub and ffmpeg for audio conversion.

## ğŸ“Œ Limitations

* Works best with general images (e.g., photos, simple graphs).
* Requires an internet connection for HuggingFace model downloads and voice APIs.
* Designed for CPU inference â€” not optimized for large-scale deployments.

## ğŸ›  Future Improvements

* ğŸ§  Swap in more powerful models like LLaVA or MiniGPT-4.
* ğŸŒ Host model and bot on cloud (Hugging Face Spaces, AWS, etc.).
* ğŸ§© Add multi-image comparison.
* ğŸ§µ Multi-turn dialogue context support.

## ğŸ¤ Contributions

PRs are welcome! Feel free to fork and enhance the assistant â€” whether by improving UX, using better models, or adding logging and metrics.








